{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6f5b2f4",
   "metadata": {},
   "source": [
    "<hr style=\"border:10px solid gray\">\n",
    "\n",
    "# Simple running example of DAHSI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8fd8c8",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "⚠️ In order to run this notebook, you need to have installed all the dependancies needed to run DAHSI as listed in the $\\texttt{README.md}$ file in this repository.\n",
    "\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "⚠️ This notebook is for illustration of the method purposes only and so the toy problem chosen contains no hidden variables to be able to go through the code in a few minutes. This notebook will give you the tools and understanding necessary to be able to build your own problem and solve it using DAHSI.\n",
    "\n",
    "ℹ️ To check an example of hidden variables, go to the `Example_LorenzSynth` folder in the github repo and explore the differences between `File1.txt` and `File2.txt` shown in this tutorial and the ones found in that folder. It is the same problem but $y$ is hidden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeb35aa",
   "metadata": {},
   "source": [
    "<hr style=\"border:10px solid gray\">\n",
    "\n",
    "## Mathematical background \n",
    "\n",
    "The algorithm data assimilation for hidden sparse inference (DAHSI) boils down to minimising the following cost function:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\label{costfunk}\n",
    "        A(\\mathbf{X},\\mathbf{p}) = \\frac{1}{N}\\sum_{i=1}^N \\Vert \\mathbf{X}(t_{i}) - \\mathbf{Y}(t_{i}) \\Vert^2 + \\frac{1}{N}\\sum_{i=1}^{N-1} R_f \\left\\{ \\Vert \\mathbf{X}(t_{i+1}) - \\mathbf{f}(\\mathbf{X}(t_i),\\mathbf{p},\\mathbf{\\hat{F}}) \\Vert^2 \\right\\} + \\lambda \\Vert \\mathbf{p} \\Vert_1.        \n",
    "\\end{equation}\n",
    "\n",
    "his function is composed of three terms: the experimental error, $A_E(\\mathbf{X},\\mathbf{Y}) = \\frac{1}{N}\\sum_{i=1}^N \\Vert \\mathbf{X}(t_{i}) - \\mathbf{Y}(t_{i}) \\Vert^2$, the model error term, $A_M(\\mathbf{X},\\mathbf{p},\\mathbf{\\hat{F}}) = \\frac{1}{N}\\sum_{i=1}^{N-1} \\left\\{ \\Vert \\mathbf{X}(t_{i+1}) - \\mathbf{f}(\\mathbf{X}(t_i),\\mathbf{p},\\mathbf{\\hat{F}}) \\Vert^2 \\right\\}$, and a sparse penalty term $\\lambda \\Vert \\mathbf{p} \\Vert_1$. Here, $\\mathbf{f}(\\mathbf{X}(t_i),\\mathbf{p},\\mathbf{\\hat{F}}) = \\mathbf{X}(t_{i+1})$ defines the discrete time model dynamics and is obtained by discretizing the governing equations using a Hermite-Simpson collocation.\n",
    "\n",
    "For full details on our method, check out our paper <a href=\"https://doi.org/10.1063/5.0066066\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439788f9",
   "metadata": {},
   "source": [
    "<hr style=\"border:10px solid gray\">\n",
    "\n",
    "We first import all llibraries we will use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd58d8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import sympy as sym\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import cyipopt\n",
    "from termcolor import colored\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import time\n",
    "import sys\n",
    "import pickle \n",
    "import os\n",
    "import pyximport\n",
    "pyximport.install()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc9f035",
   "metadata": {},
   "source": [
    "<hr style=\"border:10px solid gray\">\n",
    "\n",
    "## Generating the data \n",
    "\n",
    "We will work with the Lorenz system as it is a classical example of chaotic systems.\n",
    "\n",
    "We numerically simulate the system using Runge-Kutta 4th order with $\\Delta t = 0.01$ to obtain the <i>data</i> we will use to show a simple running example. We will consider $N = 501$ time points, and we will add to the <i>data</i> some normally distributed noise with $\\omega = 0.01$.\n",
    "\n",
    "We will save the time-series for each variable in $\\texttt{.dat}$ files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ddce6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RungeKutta4(f, t, y0, args=()):\n",
    "    n = len(t)\n",
    "    y = np.zeros((n, len(y0)))\n",
    "    y[0] = y0\n",
    "    for i in range(n - 1):\n",
    "        h = t[i+1] - t[i]\n",
    "        k1 = f(t[i], y[i], *args)\n",
    "        k2 = f(t[i] + h / 2., y[i] + k1 * h / 2., *args)\n",
    "        k3 = f(t[i] + h / 2., y[i] + k2 * h / 2., *args)\n",
    "        k4 = f(t[i] + h, y[i] + k3 * h, *args)\n",
    "        y[i+1] = y[i] + (h / 6.) * (k1 + 2*k2 + 2*k3 + k4)\n",
    "    return y\n",
    "\n",
    "def Lorenz(t, y, sigma, rho, beta):\n",
    "    return np.array([sigma * (y[1] - y[0]), \n",
    "                     y[0]*(rho - y[2]) - y[1], \n",
    "                     y[0]* y[1] - beta*y[2]])\n",
    "\n",
    "y0 = [-8.0, 7.0, 27.0]\n",
    "sigma = 10.0\n",
    "rho = 28.0\n",
    "beta = 8.0/3\n",
    "\n",
    "dt = 0.01\n",
    "N = 501\n",
    "tfin = dt * (N - 1)\n",
    "t = np.linspace(0, tfin, N)\n",
    "\n",
    "sol = RungeKutta4(Lorenz, t, y0, args=(sigma,rho,beta))\n",
    "\n",
    "# mean and standard deviation for the added Gaussian noise.\n",
    "# let's make sure we always add the same noise instance.\n",
    "np.random.seed(12345)\n",
    "mu, sigma = 0, 0.01 \n",
    "noise = np.random.normal(mu, sigma, size=(N, 3))\n",
    "\n",
    "sol = sol + noise\n",
    "\n",
    "x = sol[:,0]\n",
    "y = sol[:,1]\n",
    "z = sol[:,2]\n",
    "\n",
    "xfile = open(\"datax_Lorenz.dat\", \"w\")\n",
    "yfile = open(\"datay_Lorenz.dat\", \"w\")\n",
    "zfile = open(\"dataz_Lorenz.dat\", \"w\")\n",
    "\n",
    "for i in range(N):\n",
    "    xfile.write(\"%.5f\\n\" % x[i])\n",
    "    yfile.write(\"%.5f\\n\" % y[i])\n",
    "    zfile.write(\"%.5f\\n\" % z[i])\n",
    "\n",
    "xfile.close()    \n",
    "yfile.close()    \n",
    "zfile.close()    \n",
    "\n",
    "# plot the data we will use in DAHSI.\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "\n",
    "ax.plot(x, y, z, lw=2)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_zlabel(\"z\")\n",
    "ax.set_title(\"Lorenz Attractor\")\n",
    "\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c00fc56",
   "metadata": {},
   "source": [
    "<hr style=\"border:10px solid gray\">\n",
    "\n",
    "# A quick tour to the problem setup files\n",
    "\n",
    "First we are going to look into how we define the generic equations, parameters, bounds etc. To do so, we will load `File1.txt` and go over each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316a7dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('File1.txt', 'r') as f:\n",
    "    for i, line in enumerate(f, start=1):\n",
    "        print(colored('%.2d','green') % i, '%s' % line.strip())        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d84bf11",
   "metadata": {},
   "source": [
    "The first line contains the number of state variables (`num_vars`), parameters (`num_params`) and measured state variables (`num_meas`), the time step $\\Delta t$ and the total number of time points (`num_tpoints`), in that order and separated by commas. In the example file above, we can see that our problem has 3 state variables, 30 parameter to estimate and $3$ measured state variables. Our data consists of 501 time steps, with a step size of 0.01.\n",
    "    \n",
    "The next lines assign names to our state variables, each name in one different line. The observed variables are written first, and the hidden variables after. In this case we do not consider any hidden variables. In the example file above, the variables are named $x$, $y$ and $z$.\n",
    "\n",
    "The next lines assign names to the parameters we seek to estimate, each name in one different line. In the example provided, the parameter names are $Bk0j$, for $k=1,2,3$, and $j=1,\\dots,10$. \n",
    "    \n",
    "The next lines define the equations of our problem. One line for each equation. The equations have to be written in the same order as the state variables. For the three variables, we consider a library of monomials of the three variables up to degree two.\n",
    "    \n",
    "We now set the upper and lower bounds for the state variables, separated by a comma. One line for each state variable.\n",
    "    \n",
    "Finally, we also get the upper and lower bounds for the parameters, separated by a comma. One line for each parameter.\n",
    "\n",
    "Next we will look into `File2.txt`, were we define what our data files are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6675d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('File2.txt', 'r') as f:\n",
    "    for i, line in enumerate(f, start=1):\n",
    "        print(colored('%.2d','green') % i, '%s' % line.strip())     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65254b4",
   "metadata": {},
   "source": [
    "This file contains the information about the measured state variables.\n",
    "    \n",
    "In the first lines we give names to the data of each measured state variable. One line for each measured state variable.\n",
    "    \n",
    "The next lines include the file name (and its extension) in which we can find the data of each measured variable. One line per measured variable, and in the same order as in the first part of the file.\n",
    "    \n",
    "In the example file above we have three measured state variables: we call them `datax` and `datay` and `dataz`; we then indicate that the data of our measured state variables can be found in `datax_Lorenz.dat`, `datay_Lorenz` and `dataz_Lorenz` files.\n",
    "\n",
    "Finally, the variational annealing and model selection tuning parameters are defined in `File3.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d9ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('File3.txt', 'r') as f:\n",
    "    for i, line in enumerate(f, start=1):\n",
    "        print(colored('%.2d','green') % i, '%s' % line.strip())     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1425f3c",
   "metadata": {},
   "source": [
    "The first two lines define values for $\\alpha$ and $\\beta_{\\text{max}}$ for variational annealing. \n",
    "    \n",
    "The next two lines indicate the initial $\\lambda$ and the maximum value it can attain. This $\\lambda$ value controls the sparsity of the models recovered. We should always start with one that includes all possible functions and set a maximum $\\lambda$ for which all the terms drop to zero.\n",
    "    \n",
    "In the example file above we have set $\\alpha = 2$ and a maximum $\\beta$ value to 30. Initial $\\lambda$ is set to $10^{-6}$ and the maximum to 68."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fcd552",
   "metadata": {},
   "source": [
    "<hr style=\"border:10px solid gray\">\n",
    "\n",
    "# Compiling DAHSI\n",
    "\n",
    "⚠️ It is **absolutely** necessary to run the following line of code. \n",
    "\n",
    "`compile.py` runs three crucial scripts that enable DAHSI to run:\n",
    "* It first calculates the objective function, its Jacobian and Hessian for a general time point and saves them as strings. This is done via `Build_ObjJacHess.py`. The general expression for the objective function can be found in `Obj_Funks.py`;\n",
    "* Then it runs `WriteStrings.py` which takes the strings generated in the first script and writes them in the \"blank template\" `OneLoopInC_Blank.pyx`, which will create the functions for the objective function, its Jacobian and Hessian in *cython* format. The output is the file `OneLoopInC.pyx`;\n",
    "* Finally, it runs the file `setup.py` to create a `build` directory, a C file (`.c`), and a Shared Object file (`.so`). With this, we will be able to import our C-extension functions into our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b3f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python compile.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766f9523",
   "metadata": {},
   "source": [
    "Next, we load the variables needed from `ObjNeed.obj` created in `Build_ObjJacHess.py`. We only need the variables `row_final` and `col_final`, but by the nature of the `pickle` library, we need to load all the objects that were pickled into the file before the ones we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58471fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ObjJacHess = open('ObjJacHess.obj', 'rb') \n",
    "\n",
    "ObjFunk_Meas_eval = pickle.load(file_ObjJacHess)\n",
    "ObjFunk_Model_eval = pickle.load(file_ObjJacHess)\n",
    "Jacobian_Meas = pickle.load(file_ObjJacHess)\n",
    "Jacobian_Model = pickle.load(file_ObjJacHess)\n",
    "Hessian_Meas = pickle.load(file_ObjJacHess)\n",
    "Hessian_Model = pickle.load(file_ObjJacHess)\n",
    "row_final = pickle.load(file_ObjJacHess)\n",
    "col_final = pickle.load(file_ObjJacHess)\n",
    "nnzh = pickle.load(file_ObjJacHess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3d4594",
   "metadata": {},
   "source": [
    "We now can import all the modules and functions needed to run our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a73792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read_Files.py reads the three input text files (File1.txt, File2.txt and File3.txt).\n",
    "from Read_Files import *\n",
    "\n",
    "# Obj_Funks defines the measured and model part of the action we are minimising.\n",
    "from Obj_Funks import Meas_Funk\n",
    "from Obj_Funks import Model_Funk\n",
    "\n",
    "# We import the cost function, Jacobian and Hessian functions.\n",
    "import OneLoopInC\n",
    "from OneLoopInC import eval_f_tricky\n",
    "from OneLoopInC import eval_grad_f_tricky\n",
    "from OneLoopInC import eval_h_tricky"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46f502d",
   "metadata": {},
   "source": [
    "Finally we define a class (called DAHSI) that contains the objective function, the Jacobian and the Hessian, and define the constrains of the problem as empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e32552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAHSI():\n",
    "    def objective(self, x):\n",
    "        \"\"\"Returns the scalar value of the objective given x.\"\"\"\n",
    "        return eval_f_tricky(x,Rf)\n",
    "\n",
    "    def gradient(self, x):\n",
    "        \"\"\"Returns the gradient of the objective with respect to x.\"\"\"\n",
    "        return np.transpose(eval_grad_f_tricky(x,Rf))\n",
    "\n",
    "    def constraints(self, x):\n",
    "        \"\"\"Returns the constraints.\"\"\"\n",
    "        return array([ ], float_)\n",
    "\n",
    "    def jacobian(self, x):\n",
    "        \"\"\"Returns the Jacobian of the constraints with respect to x.\"\"\"\n",
    "        return np.array([])\n",
    "\n",
    "    # Location of the non-zero elements of the Hessian.\n",
    "    def hessianstructure(self):\n",
    "        \"\"\"Returns the row and column indices for non-zero values of the\n",
    "        Hessian.\"\"\"\n",
    " \n",
    "        return (np.array(col_final),np.array(row_final))\n",
    "\n",
    "    def hessian(self, x, lagrange, obj_factor):\n",
    "        \"\"\"Returns the non-zero values of the Hessian.\"\"\"        \n",
    "        H = eval_h_tricky(x,lagrange,obj_factor,0,Rf)        \n",
    "\n",
    "        row, col = self.hessianstructure()\n",
    "\n",
    "        return H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6691c80",
   "metadata": {},
   "source": [
    "<hr style=\"border:10px solid gray\">\n",
    "\n",
    "# Setting up the initial conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a57cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose seed for random number generation.\n",
    "# We call this IC variable the taskID number.\n",
    "IC = 0\n",
    "np.random.seed(IC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94135633",
   "metadata": {},
   "source": [
    "Use appropriate initial conditions: for observed state variables, the data provided; for hidden state variables, random; for parameters, set them all to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ff3619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounds for both state variables and parameters. \n",
    "x_L = np.ones((num_total))\n",
    "x_U = np.ones((num_total))\n",
    "\n",
    "for i in range(num_vars):\n",
    "    x_L[i:-num_params:num_vars] = float(Input1[1+2*num_vars+num_params+i].split(\",\")[0])\n",
    "    x_U[i:-num_params:num_vars] = float(Input1[1+2*num_vars+num_params+i].split(\",\")[1])\n",
    "for i in range(num_params):\n",
    "    x_L[-num_params+i] = float(Input1[1+3*num_vars+num_params+i].split(\",\")[0])\n",
    "    x_U[-num_params+i] = float(Input1[1+3*num_vars+num_params+i].split(\",\")[1])\n",
    "    \n",
    "# Initial vector.    \n",
    "x0 = (x_U-x_L)*np.random.rand(num_total)+x_L      \n",
    "for i in range(num_meas):\n",
    "    for k in range(0,num_vars*num_tpoints,num_vars):\n",
    "        x0[k+i] = data[int(k/num_vars),i] \n",
    "for i in range(num_params):    \n",
    "    x0[i-num_params] = 0\n",
    "        \n",
    "x_jp = np.zeros((num_total))    \n",
    "for i in range(num_total):\n",
    "    x_jp[i] = x0[i]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0115e17e",
   "metadata": {},
   "source": [
    "Define the problem using cyipopt (the Python wrapper around Ipopt). We also can adjust some parameters for Ipopt iself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0ced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = cyipopt.Problem(n = num_total, \n",
    "                      m = 0, \n",
    "                      problem_obj = DAHSI(), \n",
    "                      lb = x_L, \n",
    "                      ub = x_U, \n",
    "                      cl = np.array([]), \n",
    "                      cu = np.array([]))\n",
    "\n",
    "# Change some options of the Ipopt solver.\n",
    "nlp.add_option('linear_solver', 'ma97')\n",
    "nlp.add_option('mu_strategy', 'adaptive')\n",
    "nlp.add_option('adaptive_mu_globalization', 'never-monotone-mode')\n",
    "nlp.add_option('bound_relax_factor', float(0))\n",
    "nlp.add_option('print_level',0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d929e3",
   "metadata": {},
   "source": [
    "<hr style=\"border:10px solid gray\">\n",
    "\n",
    "# Running the $\\lambda$ sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd083e8",
   "metadata": {},
   "source": [
    "The core of the DAHSI algorithm is a nonlinear optimization step using VA, which is randomly initialized. At each VA step, we minimize $A_E+R_f A_M$ over state variable trajectories $\\mathbf{X}(t)$ and parameters $\\mathbf{p}$ given $R_f$ using IPOPT interfaced here via cyipopt.\n",
    "\n",
    "Now we start the loop on $\\lambda$. We start with a very small $R_f$ value and increase it as the VA step. We do this for every $\\lambda$ we want to study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aae620",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lambd = lambd_0\n",
    "\n",
    "file_name = \"D%s_M%s_IC%s_LorenzNotebook.dat\" % (num_vars, num_meas, IC) \n",
    "file_results = os.path.join(\"outputfiles\",file_name)\n",
    "f = open(file_results,\"w+\")\n",
    "\n",
    "print(colored('Variatonal annealing for different \\lambda.', attrs=['bold']))\n",
    "iter_count = 1\n",
    "# Here starts the main loop\n",
    "while lambd < lambd_max:   \n",
    "    f = open(file_results,\"a+\")\n",
    "\n",
    "    Rf0 = 1e-2\n",
    "\n",
    "    for i in range(num_total):\n",
    "        x_jp[i] = x0[i]    \n",
    "\n",
    "    print(\"Iteration #%d: \\lambda = %f\"%(iter_count,lambd))\n",
    "    for beta in tqdm_notebook(range(beta_max+1)):\n",
    "        f = open(file_results,\"a+\")\n",
    "        # Make note in results file which \\lambda and \\beta we are at.\n",
    "        f.write(\"%f %f \" % (lambd, beta))        \n",
    "\n",
    "        # Controlling how much the model is enforced.\n",
    "        Rf = Rf0*(alpha**beta)\n",
    "  \n",
    "        # Solve it via IPOPT (solution is x_jn).    \n",
    "        x_jn, info = nlp.solve(x_jp)\n",
    "            \n",
    "        obj = info['obj_val']\n",
    "        \n",
    "        # We hard threshold the parameter part of the solution (the last num_params elements).\n",
    "        for i in range(num_params):\n",
    "            if abs(x_jn[i-num_params]) < lambd:\n",
    "                x_jn[i-num_params] = 0\n",
    "\n",
    "        # We set this solution as the initial condition for the next iteration of IPOPT. \n",
    "        x_jp = x_jn   \n",
    "\n",
    "        # Write cost function value in file.\n",
    "        f.write(\"%e \" % obj)\n",
    "                \n",
    "        for k in range(num_params):\n",
    "            f.write(\"%f \" % x_jp[k-num_params])\n",
    "        f.write(\"\\n\")     \n",
    "        \n",
    "        f.close()\n",
    "\n",
    "    # Increase \\lambda value.       \n",
    "    lambd = 2*lambd\n",
    "    iter_count = iter_count+1\n",
    "f.close()   \n",
    "\n",
    "num_lambda = iter_count-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64153c9d",
   "metadata": {},
   "source": [
    "<hr style=\"border:10px solid gray\">\n",
    "\n",
    "# Basic analysis of the results\n",
    "\n",
    "We first read the output file generated by the $\\lambda$ sweep. This file is namedd `Di_Mj_ICk_LorenzNotebook.dat`, where `i` is the number of state variables, `j` is the number of measured state variables and `k` is the `taskID` we have chosen.\n",
    "    \n",
    "Each line in this file is the solution of the problem given a $\\lambda$ and a $\\beta$ value. For each $\\lambda$, we will have $\\beta_{\\text{max}} + 1$ lines. So, if we do a sweep for $N_{\\lambda}$ different $\\lambda$ values, the file will have $N_{\\lambda} (\\beta_{\\text{max}} + 1)$  lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f697ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = np.loadtxt('outputfiles/D3_M3_IC0_Lorenz.dat', unpack = False)\n",
    "\n",
    "# Convert result file into lambda, active terms.\n",
    "table_active_terms = np.zeros((num_lambda,2))\n",
    "table_check_which = np.zeros((num_lambda,num_params))\n",
    "for ll in range(num_lambda):\n",
    "    active_terms = 0\n",
    "    for i in range(num_params):\n",
    "        if output_file[(ll+1)*beta_max+ll,3+i] != 0:\n",
    "            active_terms = active_terms+1\n",
    "            table_check_which[ll,i] = 1\n",
    "    table_active_terms[ll,0] = output_file[(ll+1)*beta_max+ll,0]\n",
    "    table_active_terms[ll,1] = active_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a109d5e",
   "metadata": {},
   "source": [
    "The action plot shown below shows how the action changes with increased $\\beta$ values. We only show the action paths that correspond to the recovered correct model structure. We can observe that for high $\\beta$ the model is fully enforced (i.e., the action plot plateaus), which is a good indicator that the algorithm has worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5e5cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ll in range(num_lambda):    \n",
    "    action_lambda = output_file[(ll)*beta_max+ll:(ll+1)*beta_max+ll,2]\n",
    "    \n",
    "    if table_active_terms[ll,1] == 7:\n",
    "        plt.plot(action_lambda, marker='o', color=\"blue\", linestyle='solid', markersize=12)\n",
    "        plt.yscale(\"log\")      \n",
    "        plt.xlabel(\"$\\\\beta$\")\n",
    "        plt.ylabel(\"action\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22a2a5f",
   "metadata": {},
   "source": [
    "## Models recovered\n",
    "\n",
    "The number of active terms decreses as a function of $\\lambda$, which shows us that the sparse model selection is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb3b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ll in range(num_lambda):   \n",
    "    if table_active_terms[ll,1] == 7:\n",
    "        plt.plot(table_active_terms[ll,0], table_active_terms[ll,1], marker='o', color=\"blue\", linestyle='solid', markersize=17)\n",
    "    else:\n",
    "        plt.plot(table_active_terms[ll,0], table_active_terms[ll,1], marker='o', color=\"gray\", linestyle='solid', markersize=17)\n",
    "    \n",
    "    plt.xscale(\"log\")          \n",
    "    plt.xlabel(\"$\\lambda$\")\n",
    "    plt.ylabel(\"num. active terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a25199",
   "metadata": {},
   "source": [
    "In the following table we display the models recovered that contain 4, 7, 8 and 10 active terms. No models are found that contain 5, 6 or 9 active terms. This is because the we are doubling the $\\lambda$ at every step and this is too coarse to capture all model complexities.\n",
    "\n",
    "|   | Term | Parameter | 4 active terms | 7 active terms  | 9 active terms | 10 active terms\n",
    "|-|-|-|-|-|-|-\n",
    "| eq. $\\dot{x}$ | 1 | $p_{1,1}$ |  | **--** | -- | x\n",
    "| | $x$ | $p_{1,2}$ | x | **x** | x | x\n",
    "| | $y$ | $p_{1,3}$ | x | **x** | x | x\n",
    "| | $z$ | $p_{1,4}$ | -- | **--** | -- | --\n",
    "| | $x^2$ | $p_{1,5}$ | -- | **--** | -- | --\n",
    "| | $xy$ | $p_{1,6}$ | -- | **--** | -- | --\n",
    "| | $xz$ | $p_{1,7}$ | -- | **--** | -- | --\n",
    "| | $y^2$ | $p_{1,8}$ | -- | **--** | -- | --\n",
    "| | $yz$ | $p_{1,9}$ | -- | **--** | -- | --\n",
    "| | $z^2$ | $p_{1,10}$ | -- | **--** | -- | --\n",
    "| eq. $\\dot{y}$ | 1 | $p_{2,1}$ | -- | **--** | x | x\n",
    "| | $x$ | $p_{2,2}$ | x | **x** | x | x\n",
    "| | $y$ | $p_{2,3}$ | -- | **x** | x | x\n",
    "| | $z$ | $p_{2,4}$ | -- | **--** | -- | --\n",
    "| | $x^2$ | $p_{2,5}$ | -- | **--** | -- | --\n",
    "| | $xy$ | $p_{2,6}$ | -- | **--** | -- | --\n",
    "| | $xz$ | $p_{2,7}$ | -- | **x** | x | x\n",
    "| | $y^2$ | $p_{2,8}$ | -- | **--** | -- | --\n",
    "| | $yz$ | $p_{2,9}$ | -- | **--** | -- | --\n",
    "| | $z^2$ | $p_{2,10}$ | -- | **--** | -- | --\n",
    "| eq. $\\dot{z}$ | 1 | $p_{3,1}$ | -- | **--** | x | x\n",
    "| | $x$ | $p_{3,2}$ | -- | **--** | -- | --\n",
    "| | $y$ | $p_{3,3}$ | -- | **--** | -- | --\n",
    "| | $z$ | $p_{3,4}$ | x | **x** | x | x\n",
    "| | $x^2$ | $p_{3,5}$ | -- | **--** | -- | --\n",
    "| | $xy$ | $p_{3,6}$ | -- | **x** | x | x\n",
    "| | $xz$ | $p_{3,7}$ | -- | **--** | -- | --\n",
    "| | $y^2$ | $p_{3,8}$ | -- | **--** | -- | --\n",
    "| | $yz$ | $p_{3,9}$ | -- | **--** | -- | --\n",
    "| | $z^2$ | $p_{3,10}$ | -- | **--** | -- | --\n",
    "\n",
    "The correct model is the model with 7 active terms (highlighted in bold in the table), and its equations are\n",
    "\\begin{align}\n",
    "\\dot{x} &= p_{1,2}x + p_{1,3}y,\\\\\n",
    "\\dot{y} &= p_{2,2}x + p_{2,3}y + p_{2,7}xz,\\\\\n",
    "\\dot{z} &= p_{3,4}z + p_{3,6}xy.\n",
    "\\end{align}\n",
    "\n",
    "This is the structure of the Lorenz system, which means that we were able to recover the true system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

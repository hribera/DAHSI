{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6f5b2f4",
   "metadata": {},
   "source": [
    "<hr style=\"border:10px solid gray\">\n",
    "\n",
    "# Simple running example of DAHSI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8fd8c8",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "⚠️ In order to run this notebook, you need to have installed all the dependancies needed to run DAHSI as listed in the $\\texttt{README.md}$ file in this repository.\n",
    "\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "⚠️ This notebook is for illustration of the method purposes only and so the toy problem chosen contains no hidden variables to be able to go through the code in a few minutes. This notebook will give you the tools and understanding necessary to be able to build your own problem and solve it using DAHSI.\n",
    "\n",
    "<hr style=\"border:2px solid gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deda6678",
   "metadata": {},
   "source": [
    "<hr style=\"border:10px solid gray\">\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "(Wait until the notebook is finalised to add the table of contents.)\n",
    "\n",
    "1. [Generating the data](#Generating-the-data)\n",
    "2. [Some paragraph](#)\n",
    "    1. [Sub paragraph](#)\n",
    "3. [Another paragraph](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeb35aa",
   "metadata": {},
   "source": [
    "<hr style=\"border:10px solid gray\">\n",
    "\n",
    "## Mathematical background \n",
    "\n",
    "The algorithm data assimilation for hidden sparse inference (DAHSI) boils down to minimising the following cost function:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\label{costfunk}\n",
    "        A(\\mathbf{X},\\mathbf{p}) = \\frac{1}{N}\\sum_{i=1}^N \\Vert \\mathbf{X}(t_{i}) - \\mathbf{Y}(t_{i}) \\Vert^2 + \\frac{1}{N}\\sum_{i=1}^{N-1} R_f \\left\\{ \\Vert \\mathbf{X}(t_{i+1}) - \\mathbf{f}(\\mathbf{X}(t_i),\\mathbf{p},\\mathbf{\\hat{F}}) \\Vert^2 \\right\\} + \\lambda \\Vert \\mathbf{p} \\Vert_1.        \n",
    "\\end{equation}\n",
    "\n",
    "his function is composed of three terms: the experimental error, $A_E(\\mathbf{X},\\mathbf{Y}) = \\frac{1}{N}\\sum_{i=1}^N \\Vert \\mathbf{X}(t_{i}) - \\mathbf{Y}(t_{i}) \\Vert^2$, the model error term, $A_M(\\mathbf{X},\\mathbf{p},\\mathbf{\\hat{F}}) = \\frac{1}{N}\\sum_{i=1}^{N-1} \\left\\{ \\Vert \\mathbf{X}(t_{i+1}) - \\mathbf{f}(\\mathbf{X}(t_i),\\mathbf{p},\\mathbf{\\hat{F}}) \\Vert^2 \\right\\}$, and a sparse penalty term $\\lambda \\Vert \\mathbf{p} \\Vert_1$. Here, $\\mathbf{f}(\\mathbf{X}(t_i),\\mathbf{p},\\mathbf{\\hat{F}}) = \\mathbf{X}(t_{i+1})$ defines the discrete time model dynamics and is obtained by discretizing the governing equations using a Hermite-Simpson collocation.\n",
    "\n",
    "For full details on our method, check out our paper <a href=\"https://doi.org/10.1063/5.0066066\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439788f9",
   "metadata": {},
   "source": [
    "<hr style=\"border:10px solid gray\">\n",
    "\n",
    "We first import all llibraries we will use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd58d8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import sympy as sym\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import cyipopt\n",
    "from termcolor import colored\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import time\n",
    "import sys\n",
    "import pickle \n",
    "import os\n",
    "import pyximport\n",
    "pyximport.install()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc9f035",
   "metadata": {},
   "source": [
    "<hr style=\"border:10px solid gray\">\n",
    "\n",
    "## Generating the data \n",
    "\n",
    "We will work with the Lorenz system as it is a classical example of chaotic systems.\n",
    "\n",
    "We numerically simulate the system using Runge-Kutta 4th order with $\\Delta t = 0.01$ to obtain the <i>data</i> we will use to show a simple running example. We will consider $N = 501$ time points, and we will add to the <i>data</i> some normally distributed noise with $\\omega = 0.01$.\n",
    "\n",
    "We will save the time-series for each variable in $\\texttt{.dat}$ files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ddce6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RungeKutta4(f, t, y0, args=()):\n",
    "    n = len(t)\n",
    "    y = np.zeros((n, len(y0)))\n",
    "    y[0] = y0\n",
    "    for i in range(n - 1):\n",
    "        h = t[i+1] - t[i]\n",
    "        k1 = f(t[i], y[i], *args)\n",
    "        k2 = f(t[i] + h / 2., y[i] + k1 * h / 2., *args)\n",
    "        k3 = f(t[i] + h / 2., y[i] + k2 * h / 2., *args)\n",
    "        k4 = f(t[i] + h, y[i] + k3 * h, *args)\n",
    "        y[i+1] = y[i] + (h / 6.) * (k1 + 2*k2 + 2*k3 + k4)\n",
    "    return y\n",
    "\n",
    "def Lorenz(t, y, sigma, rho, beta):\n",
    "    return np.array([sigma * (y[1] - y[0]), \n",
    "                     y[0]*(rho - y[2]) - y[1], \n",
    "                     y[0]* y[1] - beta*y[2]])\n",
    "\n",
    "y0 = [-8.0, 7.0, 27.0]\n",
    "sigma = 10.0\n",
    "rho = 28.0\n",
    "beta = 8.0/3\n",
    "\n",
    "dt = 0.01\n",
    "N = 501\n",
    "tfin = dt * (N - 1)\n",
    "t = np.linspace(0, tfin, N)\n",
    "\n",
    "sol = RungeKutta4(Lorenz, t, y0, args=(sigma,rho,beta))\n",
    "\n",
    "# mean and standard deviation for the added Gaussian noise.\n",
    "mu, sigma = 0, 0.01 \n",
    "noise = np.random.normal(mu, sigma, size=(N, 3))\n",
    "\n",
    "sol = sol + noise\n",
    "\n",
    "x = sol[:,0]\n",
    "y = sol[:,1]\n",
    "z = sol[:,2]\n",
    "\n",
    "xfile = open(\"datax_Lorenz.dat\", \"w\")\n",
    "yfile = open(\"datay_Lorenz.dat\", \"w\")\n",
    "zfile = open(\"dataz_Lorenz.dat\", \"w\")\n",
    "\n",
    "for i in range(N):\n",
    "    xfile.write(\"%.5f\\n\" % x[i])\n",
    "    yfile.write(\"%.5f\\n\" % y[i])\n",
    "    zfile.write(\"%.5f\\n\" % z[i])\n",
    "\n",
    "xfile.close()    \n",
    "yfile.close()    \n",
    "zfile.close()    \n",
    "\n",
    "# plot the data we will use in DAHSI.\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "\n",
    "ax.plot(x, y, z, lw=2)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_zlabel(\"z\")\n",
    "ax.set_title(\"Lorenz Attractor\")\n",
    "\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c00fc56",
   "metadata": {},
   "source": [
    "<hr style=\"border:10px solid gray\">\n",
    "\n",
    "# A quick tour to the problem setup files\n",
    "\n",
    "First we are going to look into how we define the generic equations, parameters, bounds etc. To do so, we will load $\\texttt{File1.txt}$ and go over each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316a7dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('File1.txt', 'r') as f:\n",
    "    for i, line in enumerate(f, start=1):\n",
    "        print(colored('%.2d','green') % i, '%s' % line.strip())        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d84bf11",
   "metadata": {},
   "source": [
    "The first line contains the number of state variables ($\\texttt{num}\\_\\texttt{vars}$), parameters ($\\texttt{num}\\_\\texttt{params}$) and measured state variables ($\\texttt{num}\\_\\texttt{meas}$), the time step $\\Delta t$ and the total number of time points ($\\texttt{num}\\_\\texttt{tpoints}$), in that order and separated by commas. In the example file above, we can see that our problem has $3$ state variables, $30$ parameter to estimate and $2$ measured state variables. Our data consists of $501$ time steps, with a step size of $0.01$.\n",
    "    \n",
    "The next lines assign names to our state variables, each name in one different line. The observed variables are written first, and the hidden variables after. In this case we do not consider any hidden variables. In the example file above, the variables are named $x$, $y$ and $z$.\n",
    "\n",
    "The next lines assign names to the parameters we seek to estimate, each name in one different line. In the example provided, the parameter names are $Bk0j$, for $k=1,2,3$, and $j=1,\\dots,10$. \n",
    "    \n",
    "The next lines define the equations of our problem. One line for each equation. The equations have to be written in the same order as the state variables. For the three variables, we consider a library of monomials of the three variables up to degree two.\n",
    "    \n",
    "We now set the upper and lower bounds for the state variables, separated by a comma. One line for each state variable.\n",
    "    \n",
    "Finally, we also get the upper and lower bounds for the parameters, separated by a comma. One line for each parameter.\n",
    "\n",
    "Next we will look into $\\texttt{File2.txt}$, were we define what our data files are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6675d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('File2.txt', 'r') as f:\n",
    "    for i, line in enumerate(f, start=1):\n",
    "        print(colored('%.2d','green') % i, '%s' % line.strip())     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65254b4",
   "metadata": {},
   "source": [
    "This file contains the information about the measured state variables.\n",
    "    \n",
    "In the first lines we give names to the data of each measured state variable. One line for each measured state variable.\n",
    "    \n",
    "The next lines include the file name (and its extension) in which we can find the data of each measured variable. One line per measured variable, and in the same order as in the first part of the file.\n",
    "    \n",
    "In the example file above we have three measured state variables: we call them $\\texttt{datax}$ and $\\texttt{datay}$ and $\\texttt{dataz}$; we then indicate that the data of our measured state variables can be found in $\\texttt{datax}\\_\\texttt{Lorenz.dat}$, $\\texttt{datay}\\_\\texttt{Lorenz.dat}$ and $\\texttt{dataz}\\_\\texttt{Lorenz.dat}$ files.\n",
    "\n",
    "Finally, the variational annealing and model selection tuning parameters are defined in $\\texttt{File3.txt}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d9ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('File3.txt', 'r') as f:\n",
    "    for i, line in enumerate(f, start=1):\n",
    "        print(colored('%.2d','green') % i, '%s' % line.strip())     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1425f3c",
   "metadata": {},
   "source": [
    "The first two lines define values for $\\alpha$ and $\\beta_{\\text{max}}$ for variational annealing. \n",
    "    \n",
    "The next two lines indicate the initial $\\lambda$ and the maximum value it can attain. This $\\lambda$ value controls the sparsity of the models recovered. We should always start with one that includes all possible functions and set a maximum $\\lambda$ for which all the terms drop to zero.\n",
    "    \n",
    "In the example file above we have set $\\alpha = 2$ and a maximum $\\beta$ value to $30$. Initial $\\lambda$ is set to $10^{-6}$ and the maximum to $68$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fcd552",
   "metadata": {},
   "source": [
    "<hr style=\"border:10px solid gray\">\n",
    "\n",
    "# Compiling DAHSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b3f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python compile.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766f9523",
   "metadata": {},
   "source": [
    "Next, we load the variables needed from $\\texttt{ObjNeed.obj}$ created in $\\texttt{Define}\\_\\texttt{JacHess.py}$. We only need the variables $\\texttt{row}\\_\\texttt{final}$ and $\\texttt{col}\\_\\texttt{final}$, but by the nature of the $\\texttt{pickle}$ library, we need to unload all the objects that were pickled into the file before the ones we need. $\\texttt{Define}\\_\\texttt{JacHess.py}$ is the file in which we find expressions for the cost function, its Jacobian and its Hessian. Open said file to see the code thouroughly commented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58471fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ObjJacHess = open('ObjJacHess.obj', 'rb') \n",
    "\n",
    "ObjFunk_Meas_eval = pickle.load(file_ObjJacHess)\n",
    "ObjFunk_Model_eval = pickle.load(file_ObjJacHess)\n",
    "Jacobian_Meas = pickle.load(file_ObjJacHess)\n",
    "Jacobian_Model = pickle.load(file_ObjJacHess)\n",
    "Hessian_Meas = pickle.load(file_ObjJacHess)\n",
    "Hessian_Model = pickle.load(file_ObjJacHess)\n",
    "row_final = pickle.load(file_ObjJacHess)\n",
    "col_final = pickle.load(file_ObjJacHess)\n",
    "nnzh = pickle.load(file_ObjJacHess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3d4594",
   "metadata": {},
   "source": [
    "We now can import all the modules and functions needed to run our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a73792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read_Files.py reads the three input text files (File1.txt, File2.txt and File3.txt).\n",
    "from Read_Files import *\n",
    "\n",
    "# Obj_Funks defines the measured and model part of the action we are minimising.\n",
    "from Obj_Funks import Meas_Funk\n",
    "from Obj_Funks import Model_Funk\n",
    "\n",
    "# We import the cost function, Jacobian and Hessian functions.\n",
    "import OneLoopInC\n",
    "from OneLoopInC import eval_f_tricky\n",
    "from OneLoopInC import eval_grad_f_tricky\n",
    "from OneLoopInC import eval_h_tricky"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46f502d",
   "metadata": {},
   "source": [
    "Finally we define a class (called DAHSI) that contains the objective function, the Jacobian and the Hessian, and define the constrains of the problem as empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e32552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAHSI():\n",
    "    def objective(self, x):\n",
    "        \"\"\"Returns the scalar value of the objective given x.\"\"\"\n",
    "        return eval_f_tricky(x,Rf)\n",
    "\n",
    "    def gradient(self, x):\n",
    "        \"\"\"Returns the gradient of the objective with respect to x.\"\"\"\n",
    "        return np.transpose(eval_grad_f_tricky(x,Rf))\n",
    "\n",
    "    def constraints(self, x):\n",
    "        \"\"\"Returns the constraints.\"\"\"\n",
    "        return array([ ], float_)\n",
    "\n",
    "    def jacobian(self, x):\n",
    "        \"\"\"Returns the Jacobian of the constraints with respect to x.\"\"\"\n",
    "        return np.array([])\n",
    "\n",
    "    # Location of the non-zero elements of the Hessian.\n",
    "    def hessianstructure(self):\n",
    "        \"\"\"Returns the row and column indices for non-zero values of the\n",
    "        Hessian.\"\"\"\n",
    " \n",
    "        return (np.array(col_final),np.array(row_final))\n",
    "\n",
    "    def hessian(self, x, lagrange, obj_factor):\n",
    "        \"\"\"Returns the non-zero values of the Hessian.\"\"\"        \n",
    "        H = eval_h_tricky(x,lagrange,obj_factor,0,Rf)        \n",
    "\n",
    "        row, col = self.hessianstructure()\n",
    "\n",
    "        return H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6691c80",
   "metadata": {},
   "source": [
    "<hr style=\"border:10px solid gray\">\n",
    "\n",
    "# Setting up the initial conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a57cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose seed for random number generation.\n",
    "IC = 0\n",
    "np.random.seed(IC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94135633",
   "metadata": {},
   "source": [
    "Use appropriate initial conditions: for observed state variables, the data provided; for hidden state variables, random; for parameters, set them all to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ff3619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounds for both state variables and parameters. \n",
    "x_L = np.ones((num_total))\n",
    "x_U = np.ones((num_total))\n",
    "\n",
    "for i in range(num_vars):\n",
    "    x_L[i:-num_params:num_vars] = float(Input1[1+2*num_vars+num_params+i].split(\",\")[0])\n",
    "    x_U[i:-num_params:num_vars] = float(Input1[1+2*num_vars+num_params+i].split(\",\")[1])\n",
    "for i in range(num_params):\n",
    "    x_L[-num_params+i] = float(Input1[1+3*num_vars+num_params+i].split(\",\")[0])\n",
    "    x_U[-num_params+i] = float(Input1[1+3*num_vars+num_params+i].split(\",\")[1])\n",
    "    \n",
    "# Initial vector.    \n",
    "x0 = (x_U-x_L)*np.random.rand(num_total)+x_L      \n",
    "for i in range(num_meas):\n",
    "    for k in range(0,num_vars*num_tpoints,num_vars):\n",
    "        x0[k+i] = data[int(k/num_vars),i] \n",
    "for i in range(num_params):    \n",
    "    x0[i-num_params] = 0\n",
    "        \n",
    "x_jp = np.zeros((num_total))    \n",
    "for i in range(num_total):\n",
    "    x_jp[i] = x0[i]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0115e17e",
   "metadata": {},
   "source": [
    "Define the problem using cyipopt (the Python wrapper around Ipopt). We also can adjust some parameters for Ipopt iself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0ced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = cyipopt.Problem(n = num_total, \n",
    "                      m = 0, \n",
    "                      problem_obj = DAHSI(), \n",
    "                      lb = x_L, \n",
    "                      ub = x_U, \n",
    "                      cl = np.array([]), \n",
    "                      cu = np.array([]))\n",
    "\n",
    "# Change some options of the Ipopt solver.\n",
    "nlp.add_option('linear_solver', 'ma97')\n",
    "#nlp.add_option('max_iter',100)\n",
    "#nlp.add_option('tol',1.e-12)\n",
    "nlp.add_option('mu_strategy', 'adaptive')\n",
    "nlp.add_option('adaptive_mu_globalization', 'never-monotone-mode')\n",
    "# nlp.add_option('bound_relax_factor', 0)\n",
    "nlp.add_option('print_level',0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d929e3",
   "metadata": {},
   "source": [
    "<hr style=\"border:10px solid gray\">\n",
    "\n",
    "# Running the $\\lambda$ sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd083e8",
   "metadata": {},
   "source": [
    "The core of the DAHSI algorithm is a nonlinear optimization step using VA, which is randomly initialized. At each VA step, we minimize $A_E+R_f A_M$ over state variable trajectories $\\mathbf{X}(t)$ and parameters $\\mathbf{p}$ given $R_f$ using IPOPT interfaced here via cyipopt.\n",
    "\n",
    "Now we start the loop on $\\lambda$. We start with a very small $R_f$ value and increase it as the VA step. We do this for every $\\lambda$ we want to study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aae620",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lambd = lambd_0\n",
    "\n",
    "file_name = \"D%s_M%s_IC%s_Lorenz.dat\" % (num_vars, num_meas, IC) \n",
    "file_results = os.path.join(\"outputfiles\",file_name)\n",
    "f = open(file_results,\"w+\")\n",
    "\n",
    "print(colored('Variatonal annealing for different \\lambda.', attrs=['bold']))\n",
    "iter_count = 1\n",
    "# Here starts the main loop\n",
    "while lambd < lambd_max:   \n",
    "    f = open(file_results,\"a+\")\n",
    "\n",
    "    Rf0 = 1e-2\n",
    "\n",
    "    for i in range(num_total):\n",
    "        x_jp[i] = x0[i]    \n",
    "\n",
    "    print(\"Iteration #%d: \\lambda = %f\"%(iter_count,lambd))\n",
    "    for beta in tqdm_notebook(range(beta_max+1)):\n",
    "        f = open(file_results,\"a+\")\n",
    "        # Make note in results file which \\lambda and \\beta we are at.\n",
    "        f.write(\"%f %f \" % (lambd, beta))        \n",
    "\n",
    "        # Controlling how much the model is enforced.\n",
    "        Rf = Rf0*(alpha**beta)\n",
    "  \n",
    "        # Solve it via IPOPT (solution is x_jn).    \n",
    "        x_jn, info = nlp.solve(x_jp)\n",
    "            \n",
    "        obj = info['obj_val']\n",
    "        \n",
    "        # We hard threshold the parameter part of the solution (the last num_params elements).\n",
    "        for i in range(num_params):\n",
    "            if abs(x_jn[i-num_params]) < lambd:\n",
    "                x_jn[i-num_params] = 0\n",
    "\n",
    "        # We set this solution as the initial condition for the next iteration of IPOPT. \n",
    "        x_jp = x_jn   \n",
    "\n",
    "        # Write cost function value in file.\n",
    "        f.write(\"%e \" % obj)\n",
    "                \n",
    "        for k in range(num_params):\n",
    "            f.write(\"%f \" % x_jp[k-num_params])\n",
    "        f.write(\"\\n\")     \n",
    "        \n",
    "        f.close()\n",
    "\n",
    "    # Increase \\lambda value.       \n",
    "    lambd = 2*lambd\n",
    "    iter_count = iter_count+1\n",
    "f.close()   \n",
    "\n",
    "num_lambda = iter_count-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64153c9d",
   "metadata": {},
   "source": [
    "<hr style=\"border:10px solid gray\">\n",
    "\n",
    "# Basic analysis of the results\n",
    "\n",
    "NOTE: at the moment I just put these two figures without commenting the results and we can talk about which plots make sense to put and which not in our next meeting to finalise this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f697ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = np.loadtxt('outputfiles/D3_M3_IC0_Lorenz.dat', unpack = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5be44f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ll in range(num_lambda):\n",
    "#     print((ll)*beta_max+ll,(ll+1)*(beta_max)+(ll))\n",
    "# #     print(output_file[(ll)*beta_max+ll,:])\n",
    "# #     print(output_file[(ll+1)*beta_max+ll,:])\n",
    "\n",
    "# # output_file[-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaa2814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = plt.figure()\n",
    "# plt.semilogy(beta, obj, 'bo', markersize=10)\n",
    "\n",
    "# plt.rcParams[\"figure.figsize\"] = (12,10)\n",
    "# plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "# plt.xlabel(\"\\\\beta\")\n",
    "# plt.ylabel(\"log(action)\")\n",
    "# # ax.set_title(\"\")\n",
    "\n",
    "# plt.grid(True)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d760048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_array = output_file[1::,0:3]\n",
    "\n",
    "# Convert result file into first data frame with lambda, beta and action values.\n",
    "columns = [\"lambda\", \"beta\", \"action\"]\n",
    "df_1 = pd.DataFrame(data=sub_array, columns=columns)\n",
    "\n",
    "# print(df_1.head())\n",
    "\n",
    "# Convert result file into lambda, active terms.\n",
    "table_active_terms = np.zeros((num_lambda,3))\n",
    "for ll in range(num_lambda):\n",
    "#     print((ll)*beta_max+ll,(ll+1)*(beta_max)+(ll))\n",
    "    active_terms = 0\n",
    "    for i in range(num_params):\n",
    "        if output_file[(ll+1)*beta_max+ll,3+i] != 0:\n",
    "            active_terms = active_terms+1\n",
    "    table_active_terms[ll,0] = output_file[(ll+1)*beta_max+ll,0]\n",
    "    table_active_terms[ll,1] = active_terms\n",
    "    table_active_terms[ll,2] = active_terms*100\n",
    "columns_2 = [\"lambda\", \"active terms\", \"size\"]\n",
    "df_2 = pd.DataFrame(data=table_active_terms, columns=columns_2)\n",
    "\n",
    "# print(df_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0664b1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "\n",
    "sns.set(font_scale=1.75)\n",
    "\n",
    "g = sns.relplot(\n",
    "    data=df_1, kind=\"line\",\n",
    "    x=\"beta\", y=\"action\", #col=\"lambda\",\n",
    "#     hue=\"lambda\", style=\"lambda\",\n",
    "    linewidth = \"4\",\n",
    "    facet_kws=dict(sharex=False),\n",
    ")\n",
    "\n",
    "g.set_xlabels('$\\\\beta$', clear_inner=False)\n",
    "g.set_ylabels('action', clear_inner=False)\n",
    "g.set(yscale=\"log\")\n",
    "g.fig.set_size_inches(12,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f852bd",
   "metadata": {},
   "source": [
    "Comment the figure above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc473ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.75)\n",
    "\n",
    "# sns.stripplot(x = 'Pos', y = 'Weight', data = wnba, jitter = True)\n",
    "\n",
    "\n",
    "g = sns.relplot(\n",
    "    data=df_2,\n",
    "    x=\"lambda\", y=\"active terms\",\n",
    "#     hue=\"active terms\", #style=\"smoker\", size=\"size\",\n",
    "    s=300,\n",
    ")\n",
    "\n",
    "# g.set_titles(\"\")\n",
    "g.set_xlabels('$\\lambda$', clear_inner=False)\n",
    "g.set_ylabels('num. active terms', clear_inner=False)\n",
    "g.set(xscale=\"log\")\n",
    "g.fig.set_size_inches(12,10)\n",
    "# g.fig.savefig(\"seaborntest.eps\", close = True, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad6a794",
   "metadata": {},
   "source": [
    "Comment the figure above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfd72ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = [\"beta\", \"action\"]\n",
    "# for ll in range(num_lambda):\n",
    "#     table_actions = np.zeros((beta_max+1,2))\n",
    "#     table_actions[:,0] = range(31)\n",
    "#     table_actions[:,1] = output_file[(ll)*beta_max+ll:(ll+1)*(beta_max)+(ll)+1,2]\n",
    "#     df_3 = pd.DataFrame(data=table_actions, columns=columns)\n",
    "\n",
    "#     sns.set(font_scale=1.75)\n",
    "\n",
    "#     g = sns.scatterplot(\n",
    "#         data=df_3,\n",
    "#         x=\"beta\", y=\"action\",\n",
    "# #         hue=\"beta\", #style=\"smoker\", size=\"size\",\n",
    "#         s=300,\n",
    "#     )\n",
    "\n",
    "#     plt.title(\"Action path for different $\\lambda$\")\n",
    "#     plt.xlabel('$\\\\beta$')\n",
    "#     plt.ylabel('action')\n",
    "#     plt.ylim([1e-6,1e-3])\n",
    "#     g.set(yscale=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f255a936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d003d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
